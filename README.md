# Artificial Neural Networks from Scratch  

<img width="382" alt="image" src="https://github.com/user-attachments/assets/b9956cef-dc58-43ce-8a8c-914e03d298b0" />


## Overview  
Neural networks are **universal function approximators**, meaning they can model almost any function given enough capacity and training. Traditional function approximation methods, such as **piecewise linear and quadratic interpolation**, work well for structured data but their fitting capability is limited.  

In this tutorial, we start by exploring **classical function approximation techniques** and gradually transition into the **core idea of neural networks**:  
âœ… Stacking simple **linear transformations**  
âœ… Introducing **non-linearity** for flexibility  
âœ… Optimizing using **gradient descent**  

## Whatâ€™s Inside?  
- ðŸ“– **Theoretical Explanation** â€“ Why neural networks are needed beyond traditional interpolation methods
- ðŸŽ² **The "Cubed Network"** â€“ Because why settle for ReLU or sigmoid when you can go beyond with \( x^3 \)?  
- ðŸ›  **Implementation from Scratch** â€“ No TensorFlow/PyTorch, just NumPy!  
- ðŸ”„ **Training a Simple Neural Network** â€“ Understanding forward & backward propagation and gradient descent  
- ðŸ–¼ **MNIST Example** â€“ Demonstrating learning on real data  

## Why This Approach?  
Instead of jumping straight into matrix multiplications and activation functions, this tutorial **builds intuition first** by starting with **classical function approximation** and showing how neural networks naturally extend these ideas.  
